{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfca8c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the API key is loaded\n",
    "if os.getenv(\"TINKER_API_KEY\"):\n",
    "    print(\"✓ API key loaded successfully\")\n",
    "else:\n",
    "    print(\"✗ Warning: TINKER_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43aeb1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- deepseek-ai/DeepSeek-V3.1\n",
      "- deepseek-ai/DeepSeek-V3.1-Base\n",
      "- meta-llama/Llama-3.1-70B\n",
      "- meta-llama/Llama-3.1-8B\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.2-1B\n",
      "- meta-llama/Llama-3.2-3B\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "- Qwen/Qwen3-30B-A3B\n",
      "- Qwen/Qwen3-30B-A3B-Base\n",
      "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "- Qwen/Qwen3-32B\n",
      "- Qwen/Qwen3-4B-Instruct-2507\n",
      "- Qwen/Qwen3-8B\n",
      "- Qwen/Qwen3-8B-Base\n",
      "- openai/gpt-oss-120b\n",
      "- openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "import tinker\n",
    "service_client = tinker.ServiceClient()\n",
    "print(\"Available models:\")\n",
    "for item in service_client.get_server_capabilities().supported_models:\n",
    "    print(\"- \" + item.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b45a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"Qwen/Qwen3-30B-A3B-Base\"\n",
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=base_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33872b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tinker.TrainingClient object at 0x13b565b90>\n"
     ]
    }
   ],
   "source": [
    "print(training_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "565ebf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tinker-notebook/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Exception ignored in: <function InternalClientHolder.__del__ at 0x13c115260>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tinker-notebook/lib/python3.11/site-packages/tinker/lib/internal_client_holder.py\", line 231, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tinker-notebook/lib/python3.11/site-packages/tinker/lib/internal_client_holder.py\", line 226, in close\n",
      "    self.run_coroutine_threadsafe(self._async_cleanup()).result()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tinker-notebook/lib/python3.11/site-packages/tinker/lib/public_interfaces/api_future.py\", line 33, in result\n",
      "    return self._future.result(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tinker-notebook/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tinker-notebook/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tinker-notebook/lib/python3.11/site-packages/tinker/lib/internal_client_holder.py\", line 234, in _async_cleanup\n",
      "    if self._session_heartbeat_task:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'InternalClientHolder' object has no attribute '_session_heartbeat_task'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                Target               Weight    \n",
      "--------------------------------------------------\n",
      "'English'            ':'                  0.0       \n",
      "':'                  ' banana'            0.0       \n",
      "' banana'            ' split'             0.0       \n",
      "' split'             '\\n'                 0.0       \n",
      "'\\n'                 'P'                  0.0       \n",
      "'P'                  'ig'                 0.0       \n",
      "'ig'                 ' Latin'             0.0       \n",
      "' Latin'             ':'                  0.0       \n",
      "':'                  ' an'                1.0       \n",
      "' an'                'ana'                1.0       \n",
      "'ana'                '-b'                 1.0       \n",
      "'-b'                 'ay'                 1.0       \n",
      "'ay'                 ' pl'                1.0       \n",
      "' pl'                'it'                 1.0       \n",
      "'it'                 '-s'                 1.0       \n",
      "'-s'                 'ay'                 1.0       \n",
      "'ay'                 '\\n\\n'               1.0       \n"
     ]
    }
   ],
   "source": [
    "# Create some training examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"banana split\",\n",
    "        \"output\": \"anana-bay plit-say\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"quantum physics\",\n",
    "        \"output\": \"uantum-qay ysics-phay\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"donut shop\",\n",
    "        \"output\": \"onut-day op-shay\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"pickle jar\",\n",
    "        \"output\": \"ickle-pay ar-jay\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"space exploration\",\n",
    "        \"output\": \"ace-spay exploration-way\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"rubber duck\",\n",
    "        \"output\": \"ubber-ray uck-day\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"coding wizard\",\n",
    "        \"output\": \"oding-cay izard-way\"\n",
    "    },\n",
    "]\n",
    " \n",
    "# Convert examples into the format expected by the training client\n",
    "from tinker import types\n",
    " \n",
    "# Get the tokenizer from the training client\n",
    "tokenizer = training_client.get_tokenizer()\n",
    " \n",
    "def process_example(example: dict, tokenizer) -> types.Datum:\n",
    "    # Format the input with Input/Output template\n",
    "    # For most real use cases, you'll want to use a renderer / chat template,\n",
    "    # (see later docs) but here, we'll keep it simple.\n",
    "    prompt = f\"English: {example['input']}\\nPig Latin:\"\n",
    " \n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "    # Add a space before the output string, and finish with double newline\n",
    "    completion_tokens = tokenizer.encode(f\" {example['output']}\\n\\n\", add_special_tokens=False)\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    " \n",
    "    tokens = prompt_tokens + completion_tokens\n",
    "    weights = prompt_weights + completion_weights\n",
    " \n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:] # We're predicting the next token, so targets need to be shifted.\n",
    "    weights = weights[1:]\n",
    " \n",
    "    # A datum is a single training example for the loss function.\n",
    "    # It has model_input, which is the input sequence that'll be passed into the LLM,\n",
    "    # loss_fn_inputs, which is a dictionary of extra inputs used by the loss function.\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs=dict(weights=weights, target_tokens=target_tokens)\n",
    "    )\n",
    " \n",
    "processed_examples = [process_example(ex, tokenizer) for ex in examples]\n",
    " \n",
    "# Visualize the first example for debugging purposes\n",
    "datum0 = processed_examples[0]\n",
    "print(f\"{'Input':<20} {'Target':<20} {'Weight':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for i, (inp, tgt, wgt) in enumerate(zip(datum0.model_input.to_ints(), datum0.loss_fn_inputs['target_tokens'].tolist(), datum0.loss_fn_inputs['weights'].tolist())):\n",
    "    print(f\"{repr(tokenizer.decode([inp])):<20} {repr(tokenizer.decode([tgt])):<20} {wgt:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07709245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss per token: 1.0631\n",
      "Loss per token: 0.8720\n",
      "Loss per token: 0.5823\n",
      "Loss per token: 0.3476\n",
      "Loss per token: 0.1894\n",
      "Loss per token: 0.0979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(6):\n",
    "    fwdbwd_future = training_client.forward_backward(processed_examples, \"cross_entropy\")\n",
    "    optim_future = training_client.optim_step(types.AdamParams(learning_rate=1e-4))\n",
    " \n",
    "    # Wait for the results\n",
    "    fwdbwd_result = fwdbwd_future.result()\n",
    "    optim_result = optim_future.result()\n",
    " \n",
    "    # fwdbwd_result contains the logprobs of all the tokens we put in. Now we can compute the weighted\n",
    "    # average log loss per token.\n",
    "    logprobs = np.concatenate([output['logprobs'].tolist() for output in fwdbwd_result.loss_fn_outputs])\n",
    "    weights = np.concatenate([example.loss_fn_inputs['weights'].tolist() for example in processed_examples])\n",
    "    print(f\"Loss per token: {-np.dot(logprobs, weights) / weights.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d464efde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses:\n",
      "0: ' affe-coy eak-bay\\n\\n'\n",
      "1: ' offy-pay eak-break-pay\\n\\n'\n",
      "2: ' offy-cay eak-bray\\n'\n",
      "3: ' offy-ciay eak-brey\\n\\n'\n",
      "4: ' affee-ca-y ockey-bay\\n'\n",
      "5: ' offy-pay eak-bay\\n\\n'\n",
      "6: ' offer-cay eak-bray\\n\\n'\n",
      "7: ' offey -ay eak -bray\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# First, create a sampling client. We need to transfer weights\n",
    "sampling_client = training_client.save_weights_and_get_sampling_client(name='pig-latin-model')\n",
    " \n",
    "# Now, we can sample from the model.\n",
    "prompt=types.ModelInput.from_ints(tokenizer.encode(\"English: coffee break\\nPig Latin:\"))\n",
    "params = types.SamplingParams(max_tokens=20, temperature=0.0, stop=[\"\\n\"]) # Greedy sampling\n",
    "future = sampling_client.sample(prompt=prompt, sampling_params=params, num_samples=8)\n",
    "result = future.result()\n",
    "print(\"Responses:\")\n",
    "for i, seq in enumerate(result.sequences):\n",
    "    print(f\"{i}: {repr(tokenizer.decode(seq.tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4c36f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path = training_client.save_state(name=\"play_101\").result().path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf9eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinker://5b938079-2f3a-53c0-ab4c-6930dd2a5887:train:0/weights/play_101\n"
     ]
    }
   ],
   "source": [
    "print(resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81f4ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinker://5b938079-2f3a-53c0-ab4c-6930dd2a5887:train:0/sampler_weights/ephemeral_13\n"
     ]
    }
   ],
   "source": [
    "print(\"tinker://5b938079-2f3a-53c0-ab4c-6930dd2a5887:train:0/sampler_weights/ephemeral_13\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinker-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
